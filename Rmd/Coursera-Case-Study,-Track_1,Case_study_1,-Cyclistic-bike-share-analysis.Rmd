---
title: 'Cyclistic Membership Campaign'
subtitle: 'A Coursera Capstone Case-Study, Track_1,Case_study_1, Cyclistic-bike-share-analysis' 
author: 'Darrell Wolfe'
date: 'TBD'
category:
- Data Analytics
- Excl
- SQL
- R
- Rmd
- Tableau
- Coursera Data Analytics Capstone Project
output: html_document
---

## Capstone Project

The following is my Capstone Project for the Google Data Analytics Certification. The following links will take you to further information if you want to dive even deeper into the full case-study.

**Portfolio: [ToposCreative.com: Darrell's Portfolio Site](https://www.toposcreative.com/p/portfolio.html "ToposCreative.com: Darrell's Portfolio Site")**

-   **Full [Google Slides Presentation](https://docs.google.com/presentation/d/1NyPH9GfiZCNFD2Blrah0bbqqOQV1-QA1qja9m3Oug4E/edit?usp=sharing "Google Slides Presentation")**

-   **Tableau: [Tableau Capstone Presentation](https://public.tableau.com/app/profile/darrell.wolfe/viz/WolfeDarrell-GoogleDataAnalyticsProject-Option1Track1-2023_08_25/TheBusinessTask "Tableau Capstone Presentation")**

-   **Capstone Project (detailed Analysis): [Cyclistic Membership Campaign \| A Coursera Capstone Case-Study, Track_1,Case_study_1, Cyclistic-bike-share-analysis](https://www.toposcreative.com/p/cyclistic-membership-campaign-coursera.html "Cyclistic Membership Campaign | A Coursera Capstone Case-Study, Track_1,Case_study_1, Cyclistic-bike-share-analysis")**

-   **GitHub_Repo: [Coursera_Capstone_Project](https://github.com/darrellwolfe/Coursera_Capstone_Project "Coursera_Capstone_Project")**

**LinkTree: [Portfolios and Bio](https://linktr.ee/darrellwolfe "Portfolios and Bio")**

### Coursera-Case Study, Track_1,Case_study_1, Cyclistic-bike-share-analysis

As part of the [Google Data Analytics Professional Certificate](https://www.coursera.org/professional-certificates/google-data-analytics), I am to complete 1 (or more) case studies as the capstone for the course.

### Google Data Analytics Capstone: Complete a Case Study

Case Studies offered two tracks (1) chose one of two case studies to use and follow the instructions to work through the data analysis and visualization or (2) chose your own dataset. As a starter, I chose track 1, dataset 1. Having had some difficulty with a rental scooters service while I was on a business trip, the idea of working with rental bikes was compelling.

### Track 1, Case study 1, : Cyclistic bike-share analysis

Coursera offered the following summary of this dataset and case study:

> "This is an opportunity to analyze historical bicycle trip data in order to identify trends. Understanding how casual riders behave differently from riders with paid memberships is important. This analysis will help executives to make decisions about marketing programs and strategies to convert casual riders to riders with annual memberships. Refer to Case Study 1: How Does a Bike-Share Navigate Speedy Success? for more details about this case study.-- In this case study, you will perform data analysis for a fictional bike-share company in order to help them attract more riders. Along the way, you will perform numerous real-world tasks of a junior data analyst by following the steps of the data analysis process: Ask, Prepare, Process, Analyze, Share, and Act. By the time you are done, you will have a portfolio-ready case study to help you demonstrate your knowledge and skills to potential employers!"

(Note: The datasets have a different name because Cyclistic is a fictional company. For the purposes of this case study, the datasets are appropriate and will enable you to answer the business questions. The data has been made available by Motivate International Inc. under this license.) This is public data that you can use to explore how different customer types are using Cyclistic bikes.

------------------------------------------------------------------------

## Tools

Throughout this project the following tools were utilized:

-   Microsoft Excel
-   Microsoft Power Query (Excel)
-   Microsoft Data Model + DAX
-   Microsoft Power Pivot
-   Microsoft SQL Server Express
-   Microsoft Visual Studio 2022
-   Microsoft Power Point
-   Google Big Query (attempted, not used)
-   Posit RStudio
-   Tableau Public 2.1

## Programming/Analytics Languages

-   SQL
-   R
-   DAX

------------------------------------------------------------------------

# Ask, Prepare, Process, Analyze, Share and Act.

Coursera: "Along the way, you will perform numerous real-world tasks of a junior data analyst by following the steps of the data analysis process: Ask, Prepare, Process, Analyze, Share, and Act."

------------------------------------------------------------------------

## Ask

##### Guiding questions

-   What is the problem you are trying to solve?
-   How can your insights drive business decisions?

##### Key tasks

1.  Identify the business task
2.  Consider key stakeholders

##### *Deliverable: A clear statement of the business task*

In this fictitious example, Lily Moreno (Director of Marketing) has asked my marketing analytics team to help analyze historical data for a marketing campaign. The Cyclistic finance analysts have already concluded that members are more profitable than casual riders and the Moreno is convinced that the company's future success depends on maximizing memberships.

Moreno is asking three questions of her teams to "guide the future marketing program":

1.  How do annual members and casual riders use Cyclistic bikes differently?
2.  Why would casual riders buy Cyclistic annual memberships?
3.  How can Cyclistic use digital media to influence casual riders to become members?

Moreno has assigned me the first question to answer: "How do annual members and casual riders use Cyclistic bikes differently?"

She's asked for a detailed report clearly showing my findings and recommendations.

Marketing Team Goal: Design marketing strategies aimed at converting casual riders into annual members. In order to do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why casual riders would buy a membership, and how digital media could affect their marketing tactics. Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends.

------------------------------------------------------------------------

##### Business Task

Ladies and Gentlemen of the Board, the Finance Analysts have identified that annual members are more profitable than casual riders, our marketing team has taken the next step to analyze our historical data in order to develop a marketing strategy to maximize memberships.

Director Moreno has identified three key components:

1.  Investigating how annual members and casual riders utilize Cyclistic bikes differently.

2.  Unearthing the reasons that might motivate casual riders to invest in Cyclistic annual memberships.

3.  Exploring innovative ways to leverage digital media, aiming to convert casual riders into committed members.

With these actions, we intend to shape marketing strategies that resonate with our riders' preferences and patterns, increasing memberships enhancing profitability.

My task today is to address question number 1.

> "How do members and casual riders use Cyclistic differently?"

------------------------------------------------------------------------

## Prepare

##### Guiding questions

-   Where is your data located?
-   How is the data organized?
-   Are there issues with bias or credibility in this data? Does your data ROCCC?
-   How are you addressing licensing, privacy, security, and accessibility?
-   How did you verify the data's integrity?
-   How does it help you answer your question?
-   Are there any problems with the data?

##### Key tasks

1.  Download data and store it appropriately.
2.  Identify how it's organized.
3.  Sort and filter the data.
4.  Determine the credibility of the data.

##### *Deliverable: A description of all data sources used.*

### The Data: Cyclistic Bike-Share, a fictitious company.

Coursera provided a dataset from "[divvy-tripdata](https://divvy-tripdata.s3.amazonaws.com/index.html)" under public license to be used under the alias of Cyclistic Bike-Share, a fictitious company.

The data is hosted on an Amazon AWS site as multiple CSVs.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy2IbyxmBu5Kh4vx12pGMY0g5IT3ntKxH4sM9YUhFovFLYE680Lk9yufO77Xr8xKr3rNflsXaSLGFY7cefIeI6OF4rU4qb-Laghls3JmZtQYfbO4oeCI-mgvl8ReZX5_pBjG_G69XTQxrw1GVrtOBDH3j-XhTMA4wZIaCpe6pubs7vcRpVu0Pkgf9tYPTP/s1170/The_Data_CSV_Location.jpg)

------------------------------------------------------------------------

### Exploring the CSV data for Cyclistic Bike-Share, a fictitious company.

------------------------------------------------------------------------

#### Reviewing the CSV

I chose a single month CSV to explore to wrap my head around what data exists within the dataset and what issues and/or discoveries it might reveal.

-   Gathering the Data: The data comes in zip files, mostly organized by month, except for a few quarter files, but those have older data and I wanted the most current available. One of the tasks I will have is to determine how I will migrate those individual months data into a single queryable table.

-   First, I saved the CSV as a macro-enabled Excel workbook to review the data before importing it. After migrating to Macro Enabled workbook file type, I tableized the data, and brought into Power Query, then loaded it to Connection Only, Add to Data Model. This way I can see what I'm working with.

-   Second, inside the Data Model, I established a DAX formula to count distinct Ride IDs.

-   Third, I inserted several Pivot Tables into a dashboard to see what types of information slices were readily available within the dataset. I discovered Members vs Casual Riders, Bike Types, Stations, and Lat-Longs.

-   Primary Key: ride_id is the Primary Key for the dataset. It appears the numbers are 16 digits long, however, I wanted to know if any were more than 16 digits long. I took the tableized data and brought it up into Excel's native Power Query, and added a column to extract the length of the IDs. The checked the filter on the new column to look for any non-16 items. I found many IDs less than and greater than 16, sorting descending, I found the largest was 27. This gave me context to ensure that when I created a table, the Primary Key will need to have a character length of at least 27, and since this is only one of the datasets, I decided to provide buffer room and make it 60.

    -   ride_id VARCHAR(16) PRIMARY KEY *(vs)* ride_id VARCHAR(60) PRIMARY KEY,

*Note: I eventually scraped the idea of using a Primary Key in this table, which I will discuss further in this article.*

-   In this preliminary review of a single month in Excel, I discovered the following:

    -   People use classic and electric bikes about the same, and just over half of all riders are Members, the rest are casual users (almost half/half). Nearly all of the 1,256 Stations are used less than 1.00% of the time, with only one exception at Streeter Dr & G. This one stop may require further review.

        -   Casual (41.86%) vs Members (58.14%): Not a far enough gap to be significant.

        -   docked_bike is rarely used 2.08%: If this plays out should this either be eliminated or advertised more?

        -   classic_bike (43.57%) vs electric_bike (54.35%): Not a far enough gap to be significant.

        -   There are 1,265 Start/Stop stations; however, 16.1557% of all stops occured at a non-station.

            -   Only one station had stops of more than 1% of total rides, Streeter Dr & G

![Excel_DataModel_DashboardOverview](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimwEZ8i_wf5JjjOeTcuSXQdwONFNkSwPSNfiPh1Oeh9y8zEjZTh2hyDXJr61_fDOPDBBJcPLkiHn9luXgFNPrZanpjMGbp2w16NO5wi9eVXVcobQti1XOf06lqUIR_1I5bPN87PtKdNqDE9UxuhRi9l63In_L17D2JjlGgC38UCS2rTK00pJfGNoHVIb-0/s820/Excel_DataModel_DashboardOverview.jpg)

![Excel_DataModel_Dashboard_Stations 1](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFcIsfYjrkcxo42hVQBgEYPFSrQ8Q4i2betmnNexmzdY6TbBLdNuMRNSqi613QYqPAN3C40uXzYivixsCcePn7F6ANQ3X540JtpNJO-9QRQFAut8-4kdgscTsacJv9WHIGpjVBR5uZfRqOqMkIgDW8gZE0jdk07dFsFG_LsB4_W6H5p1N_WMev9mPkMA5Q/s1173/Excel_DataModel_Dashboard_Stations%201.jpg)

![Excel_DataModel_Dashboard_Stations 2](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjPzoVhGXEIUCBu9W68ycvYEQTOHOzsUEgeRclqbLZZcZwtvhIcALLH1gJf1ZY2Yicw-pTyWNUiNhN56CN9EqqYC1ErJu6_ee21aZBkiB-M6aw-veWqpkgL3jWbEMJqukbvzwu2KG1CjRcDtXvZHe60suKajqif75v56rE9EaWSANSWtE8e9uCkAVk-Wnca/s412/Excel_DataModel_Dashboard_Stations%202.jpg)

------------------------------------------------------------------------

### Gathering and importing the data:

Google's Big Query would not take even a single CSV, as the file size was too large. I was not interested in paying for Google Cloud to do this project, so I needed to find another option. While a CSV can hold multiple millions of rows of data, and Tableau can handle at least 10 million rows of data, Excel has a limit of 1,048,576 rows and 16,384 columns per sheet. Therefore, combining the 19 monthly files (January 1, 2022 through July 31, 2023) required a new work-around.

------------------------------------------------------------------------

#### Solution: Establishing a local database to work with the data

I set up a database on my local PC, created a table, and imported the data into my own localized database for analysis.

*Tools/Languages: Visual Studio 2022, SQL, CSVs on my local drive*

1.  I used Visual Studio 2022 to establish a new database on my local PC: "Coursera_Capstone_Project".
2.  I created a table on the new database for this data: "Cyclistic_divvy_tripdata"
3.  I stored the CSVs in a dedicated location on my local PC.
4.  I used a SQL script to import the CSVs into the table (with a few kinks to work out along the way)

------------------------------------------------------------------------

##### 1 Created a new database

Using Visual Studio 2022, I created a database dedicated to this project (and possibly others).

![Visual Studio 2022 New Database](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKBE_Rpm9qyz0mRToG2MkmEHA5l-2AvPGSDIGyLDCK1yoAfr6uvWT0xk4zxbPZeyvEZrQHmiO1RLgaQE0TvbURbcxU1OUGw8H7uETvMz43LxmSVBe9ZaPVwD9QnA6Jj1aVnDpjrqezKGrp3fupjkWojwE4W5iJ4y2qYq0sS2gq7DruvyZZSvMudIWEg_09/s334/VisualStudio2022_CreateDatabase.jpg)

------------------------------------------------------------------------

##### 2 SQL Scripting the new table

By copying the CSV headers into my Visual Studio 2022 code editor, and them using them as a basis for the new table, I was able to create a table situated to received the CSV data.

*Note: After working with the data, I deleted and rebuilt the table with new specifications, including getting rid of the primary key.*

```{sql create-table, eval=FALSE, include=TRUE}
-- !preview conn=conn

CREATE TABLE [dbo].[Cyclistic_divvy_tripdata] (
    ride_id VARCHAR(60), 
    rideable_type VARCHAR(MAX),
    started_at DATETIME,
    ended_at DATETIME,
    start_station_name VARCHAR(MAX),
    start_station_id VARCHAR(MAX),
    end_station_name VARCHAR(MAX),
    end_station_id VARCHAR(MAX),
    start_lat DECIMAL(22, 20),
    start_lng DECIMAL(22, 20),
    end_lat DECIMAL(22, 20),
    end_lng DECIMAL(22, 20),    
    member_casual VARCHAR(MAX)
);

--  PRIMARY KEY became an issue while importing data, removing Primary Key requirement for this table. I handled this by removing duplicates later.

```

------------------------------------------------------------------------

#### 3. Importing the CSVs into the database

To accomplish this step, I ran a BULK INSERT in SQL via Visual Studio 2022, replacing each `FROM` line with the directions to the individual CSVs. After checking with my resources, I did find a way to use Power Shell to automated this; however, I didn't understand it well enough for this task. I'll be working on learning Power Shell commands for future use.

*Tools/Languages: Visual Studio 2022, SQL, CSVs*

**Notes about RStudio, SQL, and why I worked with Visual Studio 2022 for this step.** *I originally started this step inside RStudio with a connection to the database and new table. However, I found that RStudio (at least with my current settings) would not indicate whether the SQL script was still running. A short hardly-noticeable flash would be the only indication the script was complete. Because of this, I frequently found myself trying to run the next script and getting an error that the connection was busy. Because of this, I also couldn't run a query to see if the newest data was present (because the connection was still busy writing it). Due to this ambiguity, I found that it was easier use Visual Studio 2022 to write the CSVs into the database, as it is designed for exactly this type of task.*

I ran into two separate issues inserting the data into the new table.

1.  Duplicate data broke my Primary Key, so I rebuilt the table without it and sorted out duplicate data in subsequent steps.

2.  CSV formatting `m/d/yyyy hh:mm`. In the first several CSVs, this was the native custom formatting for the columns `started_at` and `ended_at` as they came from the downloads. However, several subsequent CSVs were formatted as "General" without this custom formatting. As a result, the SQL script would code an error for mismatched types. To solve this issue, I copied the formatting into these CSVs, re-saved, and re-ran the scripts with no errors.

-   Sometimes this was caused by the ride_id showing up in one month because it "started" in that month but the ride ended the next day in the next month. The insertion process also caused several NULL rows. Both were sorted out in subsequent steps.

3.  Refreshing the view of the data, sorted by date, DESC allowed me to confirm the new data had completed before moving on to the next CSV.

##### BULK INSERT SQL Script

```{sql bulk-insert, eval=FALSE, include=TRUE}
BULK INSERT Cyclistic_divvy_tripdata
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202209-divvy-tripdata.csv'

WITH (
   FIELDTERMINATOR = ',',
   ROWTERMINATOR = '\n',
   FIRSTROW = 2
);

/*
I replaced the line with the new CSV for each of the months Jan 2022 through Aug 2023
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202201-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202202-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202203-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202204-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202205-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202206-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202207-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202208-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202209-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202210-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202211-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202212-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202301-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202302-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202303-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202304-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202305-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202306-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202307-divvy-tripdata.csv'
FROM 'C:\Users\darre\OneDrive\Documents\!Datasets\Cyclistic_divvy_tripdata CSVs\CSVs\202308-divvy-tripdata.csv'
*/


```

##### BULK INSERT SQL Script ERROR

![VisualStudio2022_BULKINSERT_ERROR](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQKE5WvVIVFvgMlcli63UU1K7DLke7yAjgA7pfFmfEfZkGSSbq_uZFzXCCAZ3dblP1WaOqVm6-0T5kBk9L8afXa8KXKX95VBXrbenYQwNFc8o5lPFcE0Bu6Op43Ng2qD1574cr_aBINA2ZXpy9z7uAHUBeA9SrawYpT12keXW7ifzsbP6sm4f5PN5FmDM8/s1921/VisualStudio2022_BULK_INSERT_ERRORs.jpg)

##### BULK INSERT SQL Script: View Data

![VisualStudio2022_ViewData](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEEMbbvwMdIJnR4qjtHJ6JbHdEfKdJaFS0lQM-4bFZ_Ed0ewJRtiw1lX1t3tO-YCG-jPPs53cJEHyqamSvfwksgJ3BZek5fiVZk9K06xPAa1J1lo-QPfQuyvmCQdfU6593qStZ9ImmF8uU6QRQTS7zNutqElGcPzdEPCKQZe_o0jqYukUPl-2d0yHn6BAj/s1031/VisualStudio2022_ViewData.jpg)

------------------------------------------------------------------------

## Process

##### Guiding questions

-   What tools are you choosing and why?
-   Have you ensured your data's integrity?
-   What steps have you taken to ensure that your data is clean?
-   How can you verify that your data is clean and ready to analyze?
-   Have you documented your cleaning process so you can review and share those results?

##### Key tasks

1.  Check the data for errors.
2.  Choose your tools.
3.  Transform the data so you can work with it effectively.
4.  Document the cleaning process.

##### *Deliverable:Documentation of any cleaning or manipulation of data*

------------------------------------------------------------------------

### Transformations & Clean Up

With all the data reviewed and imported in previous steps, I began adding transformations and clean up steps that would make the data easier to work with. After having issues with the Primary Key in previous steps, I set out to remove duplicates and NULLs from the dataset. I then began working with my data to add relevant extrapolations from the existing material.

Still working in SQL inside of Visual Studio 2022, I worked through the following transformations.

##### Clean up and transformation SQL Scripts

```{sql read-scripts, eval=FALSE, include=TRUE}

-- These read scripts allowed me to continually check my work
-- Several other approaches were attempted but these proved the most useful.

-- High Level Overview
SELECT TOP 10 *
FROM dbo.Cyclistic_divvy_tripdata
ORDER BY started_at DESC

-- Counts as checks
SELECT
rideable_type,
COUNT(ride_id) AS Trips
FROM dbo.Cyclistic_divvy_tripdata
GROUP BY rideable_type;


/*
CLEAN UP STEPS
*/

-- Removed rows with all NULL values

DELETE FROM [dbo].[Cyclistic_divvy_tripdata]
WHERE ride_id IS NULL
  AND rideable_type IS NULL
  AND started_at IS NULL
  AND ended_at IS NULL
  AND start_station_name IS NULL
  AND member_casual IS NULL;

-- Removed duplicate ride_id, as these are assumed to be unique to each ride.

WITH CTE AS (
  SELECT 
    ROW_NUMBER() OVER (PARTITION BY ride_id ORDER BY ride_id) AS rn,
    *
  FROM [dbo].[Cyclistic_divvy_tripdata]
)
DELETE FROM CTE WHERE rn > 1;


-- This read script allowed for:
-- checking for duplicate ride_id since this was part of the reason my bulk inserts weren't working according to the error messages
-- Once I came to zero results, I knew my clean-up was working.
WITH CTE AS (
  SELECT 
    ROW_NUMBER() OVER (PARTITION BY ride_id ORDER BY ride_id) AS rn,
    *
  FROM [dbo].[Cyclistic_divvy_tripdata]
)

SELECT *
FROM CTE
WHERE CTE.rn > 1



/*
TRANSFORMATION UP STEPS
*/

-- Added columns for a month number and year extrapolated from the start date
ALTER TABLE [dbo].[Cyclistic_divvy_tripdata]
ADD month_started_at INT,
    year_started_at INT;
    
UPDATE [dbo].[Cyclistic_divvy_tripdata]
SET month_started_at = MONTH(started_at),
    year_started_at = YEAR(started_at);


-- Added columns for ride_length and day of the week, 
ALTER TABLE [dbo].[Cyclistic_divvy_tripdata]
ADD ride_length INT,
    day_of_week VARCHAR(9);

UPDATE [dbo].[Cyclistic_divvy_tripdata]
SET ride_length = DATEDIFF(MINUTE, started_at, ended_at), -- in minutes
    day_of_week = DATENAME(WEEKDAY, started_at); -- returns the day of the week



-- Added column for Month Name (as opposed to month number above)
ALTER TABLE [dbo].[Cyclistic_divvy_tripdata]
ADD month_name VARCHAR(20);

UPDATE [dbo].[Cyclistic_divvy_tripdata]
SET month_name = DATENAME(MONTH, started_at);

-- GPT-4 tried to follow this with from parts, but we have the original date, so I changed this to be simpler.
-- UPDATE [dbo].[Cyclistic_divvy_tripdata]
-- SET month_name = DATENAME(MONTH, DATEFROMPARTS(year_started_at, month_started_at, 1));



--Added column for the season the months are involved with, as this related to Bike Travel
ALTER TABLE [dbo].[Cyclistic_divvy_tripdata]
ADD season VARCHAR(20);

UPDATE [dbo].[Cyclistic_divvy_tripdata]
SET season = 
  CASE 
    WHEN MONTH(started_at) IN (3, 4, 5) THEN 'Spring'
    WHEN MONTH(started_at) IN (6, 7, 8) THEN 'Summer'
    WHEN MONTH(started_at) IN (9, 10, 11) THEN 'Autumn'
    ELSE 'Winter'
  END;
  
```

------------------------------------------------------------------------

## Analyze

##### Guiding questions

-   How should you organize your data to perform analysis on it?
-   Has your data been properly formatted?
-   What surprises did you discover in the data?
-   What trends or relationships did you find in the data?
-   How will these insights help answer your business questions?

##### Key tasks

1.  Aggregate your data so it's useful and accessible.
2.  Organize and format your data.
3.  Perform calculations.
4.  Identify trends and relationships.

##### *Deliverable:A summary of your analysis*

------------------------------------------------------------------------

### Migrating to RStudio for Analysis

With the table now ready for review, I then established a connection to the database in RStudio. Some of the above steps were actually completed after this step began, as I discovered more ways to slice the data (Ex: Adding Seasons)

When working with RStudio connected to a live database, you can open SQL Scripts with `-- !preview conn=conn`, which allow you to then view the results of the SQL within RStudio. This step is important in RStudio, as it allows the SQL Script to speak directly with the database. I worked in both SQL and R while working inside RStudio. However, I found most of the analysis steps easier in R, which is what I will present below.

------------------------------------------------------------------------

#### Establishing a database connection in RStudio

These packages are not all required for the connection, but were eventually helpful in my analysis, so if you are repeating these steps, you will want to use them.

------------------------------------------------------------------------

##### Install Packages

The following packages will allow us to work with the database and the data.

```{r install-packages, eval=FALSE, include=TRUE}
install.packages("DBI")
install.packages("odbc")
install.packages("RODBC")
install.packages("tidyverse")
install.packages("ggplot2")
install.packages("geosphere")

```

------------------------------------------------------------------------

##### Establish a Data Frame: Import the data from the database into RStudio

Load the libraries, Establish a Connection, Assign an alias to the database table.

```{r library, eval=TRUE, include=TRUE}
# LOAD LIBRARIES
library(DBI)
library(odbc)
library(RODBC)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(scales)
library(geosphere)


connection <- odbcDriverConnect("driver={SQL Server};server=LAPTOP-76LHVPRQ\\SQLEXPRESS;database=Coursera_Capstone_Project;trusted_connection=true")

Cyclistic_df <- sqlFetch(connection, "dbo.Cyclistic_divvy_tripdata")

# This did not work:Server = "LAPTOP-76LHVPRQ\SQLEXPRESS",
# That worked! Server = "LAPTOP-76LHVPRQ\\SQLEXPRESS",
# One of the quirks of R is that some items you expect to work `=` require double `==`.
# Backslashes must be doubled `\` becomes `\\`. I also learned changing them to forward slashes works as well for CSV file paths.
```

*Note: At first, this code would not work, I checked my resources, and found that even though the actual file path has one backslash, the R code required a double backslash, in much the same way it requires `==` when using logical code.*

![RStudio_Database_Connection](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEit69kcpgx0KFlcrExl3ruir4c8v2-gSmukfx5C1_AqTJx7txNRume-jg4jR2uyOsY4wq5QIDQ1AhBKfKd8lzgs6-EcqW8u9uY-pfQ54MIoRFmnw1USys2WZ3lI8oZzdvBQKKBbwmwJ1gnoDzOKEv9HyD31pMa2atNolGj1-Uit9-Rk55ZoCRyrb4dyzbr2/s378/RStudio_Database_Connection.jpg)

------------------------------------------------------------------------

##### Write the final table (or smaller tailored data views) to a CSV for use in Tableau or Power BI or Looker

```{r write-csv, eval=FALSE, include=TRUE}

write.csv(Cyclistic_df, "C:/Users/darre/OneDrive/Documents/!Datasets/Cyclistic_divvy_tripdata CSVs/Final Dataset/Cyclistic_df.csv", row.names = FALSE)

```

------------------------------------------------------------------------

##### Example: SQL inside RStudio

This R code will establish the connection to the database inside R.

```{r connect-database, eval=FALSE, include=TRUE}

conn <- dbConnect(odbc(),
    Driver = "SQL Server",
    Server = "LAPTOP-76LHVPRQ\\SQLEXPRESS",
    Database = "Coursera_Capstone_Project",
    Trusted_Connection = "True")
```

###### SQL inside RStudio

This will only work after a connection to the database is established.

```{sql select-top, eval=FALSE, include=TRUE}

-- !preview conn=conn

SELECT TOP 10 *
FROM dbo.Cyclistic_divvy_tripdata
ORDER BY started_at DESC;

```

------------------------------------------------------------------------

##### Analysis: Initial Exploration, verifying the dataset

These worked as a proof that the data was loaded and showed me some initial calculations were working.

-   colnames() - Reveals the column names of the dataset. Very handy to make sure you have the final dataset with all the transformations added in the SQL step.

-   str() - Reveals the structure of the data, which can serve as a diagnostic tool, or an aide to help find clues in the dataset.

```{r column-names, eval=TRUE, include=TRUE}
colnames(Cyclistic_df)
```

```{r structure, eval=TRUE, include=TRUE}
str(Cyclistic_df)
```

------------------------------------------------------------------------

##### Analysis: High Level Overview

-   nrow(): Total rows, which is total entries and ride_ids.

-   max(): The longest ride in the dataset.

-   mean(): The mean-average of all the ride times.

-   median(): The middle ride of the ride times.

```{r nrow, eval=TRUE, include=TRUE}
nrow(Cyclistic_df)

total_rides <- nrow(Cyclistic_df)
```

```{r max, eval=TRUE, include=TRUE}
max(Cyclistic_df$ride_length)
```

```{r mean, eval=TRUE, include=TRUE}
mean(Cyclistic_df$ride_length)
```

```{r median, eval=TRUE, include=TRUE}
median(Cyclistic_df$ride_length)
```

------------------------------------------------------------------------

##### Analysis: Ride Length by Member vs Causal

-   Max:

-   Mean:

-   Median:

```{r max-ride, eval=TRUE, include=TRUE}
Cyclistic_df %>%
  group_by(member_casual) %>%
  summarise(max_ride_length = max(ride_length, na.rm = TRUE))
```

```{r mean-ride, eval=TRUE, include=TRUE}
Cyclistic_df %>%
  group_by(member_casual) %>%
  summarise(mean_ride_length = mean(ride_length, na.rm = TRUE))
```

```{r median-ride, eval=TRUE, include=TRUE}
Cyclistic_df %>%
  group_by(member_casual) %>%
  summarise(median_ride_length = median(ride_length, na.rm = TRUE))
```

------------------------------------------------------------------------

##### Analysis: Ride Length by Member vs Causal, Split by Day of the Week

-   Max:

-   Mean:

-   Median:

```{r max-ride-day, eval=TRUE, include=TRUE}
Cyclistic_df %>%
  group_by(member_casual, day_of_week) %>%
  summarise(max_ride_length = max(ride_length, na.rm = TRUE))
```

```{r mean-ride-day, eval=TRUE, include=TRUE}
Cyclistic_df %>%
  group_by(member_casual, day_of_week) %>%
  summarise(mean_ride_length = mean(ride_length, na.rm = TRUE))
```

```{r median-ride-day, eval=TRUE, include=TRUE}
Cyclistic_df %>%
  group_by(member_casual, day_of_week) %>%
  summarise(median_ride_length = median(ride_length, na.rm = TRUE))
```

------------------------------------------------------------------------

##### Analysis: Ride Length by Member vs Casual, split by Rideable Type

-   Max:

-   Mean:

-   Median:

```{r max-ride-type, eval=TRUE, include=TRUE}
Cyclistic_df %>%
  group_by(member_casual, rideable_type) %>%
  summarise(max_ride_length = max(ride_length, na.rm = TRUE))
```

```{r mean-ride-type, eval=TRUE, include=TRUE}
Cyclistic_df %>%
  group_by(member_casual, rideable_type) %>%
  summarise(mean_ride_length = mean(ride_length, na.rm = TRUE))
```

```{r median-ride-type, eval=TRUE, include=TRUE}
Cyclistic_df %>%
  group_by(member_casual, rideable_type) %>%
  summarise(median_ride_length = median(ride_length, na.rm = TRUE))
```

------------------------------------------------------------------------

##### Analysis: Ride Length by Member vs Casual, Split by Month & Season

-   Max:

-   Mean:

-   Median:

```{r view-data, eval=FALSE, include=TRUE}

view(Cyclistic_df %>%
       group_by(member_casual, month_started_at, month_name, season) %>%
       summarise(max_ride_length = max(ride_length, na.rm = TRUE)))

view(Cyclistic_df %>%
       group_by(member_casual, month_started_at, month_name, season) %>%
       summarise(mean_ride_length = mean(ride_length, na.rm = TRUE)))

view(Cyclistic_df %>%
       group_by(member_casual, month_started_at, month_name, season) %>%
       summarise(median_ride_length = median(ride_length, na.rm = TRUE)))
```

------------------------------------------------------------------------

##### Assigned as dataframes: Ride Length by Member vs Casual, Split by Month & Season

```{r assigned-df, eval=TRUE, include=TRUE}
Members_Month_MAX <- Cyclistic_df %>%
  group_by(member_casual, month_started_at, month_name, season) %>%
  summarise(max_ride_length = max(ride_length, na.rm = TRUE))

Members_Month_Mean <- Cyclistic_df %>%
  group_by(member_casual, month_started_at, month_name, season) %>%
  summarise(mean_ride_length = mean(ride_length, na.rm = TRUE))

Members_Month_Median <- Cyclistic_df %>%
  group_by(member_casual, month_started_at, month_name, season) %>%
  summarise(median_ride_length = median(ride_length, na.rm = TRUE))

```

------------------------------------------------------------------------

##### Analysis: Plot Point Graph using dataframes

-   Max:

-   Mean:

-   Median:

```{r ggplot-max, eval=TRUE, include=TRUE}
ggplot(data = Members_Month_MAX) +
  geom_point(mapping = aes(x = season, y = max_ride_length, color = member_casual, size = .25))
```

```{r ggplot-mean, eval=TRUE, include=TRUE}
ggplot(data = Members_Month_Mean) +
  geom_point(mapping = aes(x = season, y = mean_ride_length, color = member_casual, size = .25))
```

```{r ggplot-median, eval=TRUE, include=TRUE}
ggplot(data = Members_Month_Median) + 
  geom_point(mapping = aes(x = season, y = median_ride_length, color = member_casual, size = .25))

  #  geom_smooth(mapping = aes(x = season, y = median_ride_length, color = member_casual), method = lm, se = FALSE)
```

------------------------------------------------------------------------

##### Analysis: Bar Chart graph using dataframes

-   Bar Chart:

```{r bar-chart-compare-member-to-casual, eval=TRUE, include=TRUE}
ggplot(Cyclistic_df, aes(x = member_casual)) +
  geom_bar() +
  geom_text(aes(label = sprintf("%s", comma(..count..))), stat = 'count', vjust = -0.5)
```

------------------------------------------------------------------------

##### Analysis: Save Images

This saves the final product as an image to work with later.

```{r save-images, eval=FALSE, include=TRUE}
ggsave("Mean_Ride_Length.jpg")
ggsave("Max_Ride_Length.jpg")
ggsave("Median_Ride_Length.jpg")
ggsave("Bar_Members_vs_Casual.jpg")
```

##### Analysis: Stations used by Members vs Casual riders

When viewing the data I found several interesting points of reference.

-   When the entire dataset is viewed in map, Members and Casual riders use the basic regions, no clear region stood out.
-   When the data is looked at closer; however, most stations (and non-stations) have fewer than 1% of the total stops/starts.
-   By filtering for the top 25 stations, a trend appears by comparing bar charts and mapped data in Tableau.
    -   Casual Riders: Use the stations closer to the water and tourist attractions.
    -   Members most frequently use the stations further inland in major urban areas.
    -   With this, we can suggest that we focus on casual riders at the stations most used by members in these inland areas, and specifically start with a pilot at one station showing promise.

```{r eval=FALSE, include=TRUE}
stations_df <- sqldf("select
          start_station_name as station_name, 
          start_station_id as station_id,
          start_lat as lat,
          start_lng as long,
          member_casual,
          count(start_station_name) as station_stops
       from Cyclistic_df 
       group by
          start_station_name, 
          start_station_id,
          start_lat,
          start_lng,
          member_casual

      union

                    select
          end_station_name as station_name, 
          end_station_id as station_id,
          end_lat as lat,
          end_lng as long,
          member_casual,
          count(end_station_name) as station_stops
       from Cyclistic_df 
       group by
          end_station_name, 
          end_station_id,
          end_lat,
          end_lng,
          member_casual
 
      order by station_stops desc")

```

##### Analysis: Farthest Distance Traveled

I wanted to see how far the farthest rides were. When I built this calculation, adding the distance column via R script, I ran into a few issues.

```{r new-column-distance, eval=FALSE, include=TRUE}
# Distance in Miles
# Applying the distance calculation to each row and converting to miles
Cyclistic_df$distance_traveled_miles <- mapply(function(lat1, lon1, lat2, lon2) {
  distHaversine(c(lon1, lat1), c(lon2, lat2)) * 0.000621371
}, lat1 = Cyclistic_df$start_lat, lon1 = Cyclistic_df$start_lng, lat2 = Cyclistic_df$end_lat, lon2 = Cyclistic_df$end_lng)
# Now the Cyclistic_df will have a new column 'distance_traveled' with the distances in miles.

str(Cyclistic_df)
summary(Cyclistic_df$start_lat)
summary(Cyclistic_df$start_lng)
summary(Cyclistic_df$end_lat)
summary(Cyclistic_df$end_lng)
```

------------------------------------------------------------------------

When I attempted to run mean, median, and max distances, I discovered NA values in the end-lat/long data. So I removed NA rows as a new dataframe for this calculation.

> mean(Cyclistic_df$distance_traveled_miles) [1] NA median(Cyclistic_df$distance_traveled_miles) [1] NA max(Cyclistic_df$distance_traveled_miles) [1] NA summary(Cyclistic_df$start_lat) Min. 1st Qu. Median Mean 3rd Qu. Max. 41.64 41.88 41.90 41.90 41.93 45.64 summary(Cyclistic_df$start_lng) Min. 1st Qu. Median Mean 3rd Qu. Max. -87.92 -87.66 -87.64 -87.65 -87.63 -73.80 summary(Cyclistic_df$end_lat) Min. 1st Qu. Median Mean 3rd Qu. Max. NA's 0.00 41.88 41.90 41.90 41.93 42.37 8862 summary(Cyclistic_df\$end_lng) Min. 1st Qu. Median Mean 3rd Qu. Max. NA's -88.16 -87.66 -87.64 -87.65 -87.63 0.00 8862

```{r remove-na, eval=FALSE, include=TRUE}

# Removing rows with NA values
`Cyclistic_df_No_NA <- na.omit(Cyclistic_df)`

summary(Cyclistic_df_No_NA$start_lat)
summary(Cyclistic_df_No_NA$start_lng)
summary(Cyclistic_df_No_NA$end_lat)
summary(Cyclistic_df_No_NA$end_lng)

# Explore the df: What are the mean, median, min, and max?
`mean(Cyclistic_df_No_NA$distance_traveled_miles)`
`median(Cyclistic_df_No_NA$distance_traveled_miles)`
`max(Cyclistic_df_No_NA$distance_traveled_miles)`

`view(sqldf("select * from Cyclistic_df_No_NA where distance_traveled_miles > 6100"))`

```

------------------------------------------------------------------------

After removing NA, I still found oddities that didn't make sense. Several rides were over 6,100 miles in 15 minutes. That is impossible. After looking at the data, I found these rides had start lat/longs but '0' for ending lat/longs. To solve this I created a second dataframe (preserving the former in case something went wrong, which it did because I had == instead of !=).

```{r remove-zeros, eval=FALSE, include=TRUE}
# Removing rows with 0 values in the lat long columns.
Cyclistic_df_No_Zero <- Cyclistic_df_No_NA[Cyclistic_df_No_NA$start_lat != 0 & Cyclistic_df_No_NA$start_lng != 0 & Cyclistic_df_No_NA$end_lat != 0 & Cyclistic_df_No_NA$end_lng != 0,]

summary(Cyclistic_df_No_Zero$start_lat)
summary(Cyclistic_df_No_Zero$start_lng)
summary(Cyclistic_df_No_Zero$end_lat)
summary(Cyclistic_df_No_Zero$end_lng)

# Explore the df: What are the mean, median, min, and max?
mean(Cyclistic_df_No_Zero$distance_traveled_miles)

median(Cyclistic_df_No_Zero$distance_traveled_miles)

max(Cyclistic_df_No_Zero$distance_traveled_miles)

```

------------------------------------------------------------------------

With NA and 0 values removed, I can run the results.

> summary(Cyclistic_df_No_Zero$start_lat) Min. 1st Qu. Median Mean 3rd Qu. Max. 41.65 41.88 41.90 41.90 41.93 45.64 summary(Cyclistic_df_No_Zero$start_lng) Min. 1st Qu. Median Mean 3rd Qu. Max. -87.84 -87.66 -87.64 -87.64 -87.63 -73.80 summary(Cyclistic_df_No_Zero$end_lat) Min. 1st Qu. Median Mean 3rd Qu. Max. 41.65 41.88 41.90 41.90 41.93 42.06 summary(Cyclistic_df_No_Zero$end_lng) Min. 1st Qu. Median Mean 3rd Qu. Max. -87.84 -87.66 -87.64 -87.65 -87.63 -87.53

Which then gave me the three figures I was seeking.

> mean(Cyclistic_df_No_Zero\$distance_traveled_miles) [1] 1.291893
>
> median(Cyclistic_df_No_Zero\$distance_traveled_miles) [1] 0.9647673
>
> max(Cyclistic_df_No_Zero\$distance_traveled_miles) [1] 739.9625

```{r distance-figures, eval=FALSE, include=TRUE}

# Explore the df: What are the mean, median, min, and max?
mean(Cyclistic_df$distance_traveled_miles)

median(Cyclistic_df$distance_traveled_miles)

max(Cyclistic_df$distance_traveled_miles)

```

------------------------------------------------------------------------

### Analysis: Tableau

After exporting several views from SQL and R into CSV, then importing them into Tableau Public, I found several interesting insights. Primarily, these insights came from splicing and slicing the data into different views, to see if anything visually jumped off the page.

Click this link to view the: [The Tableau Public Presentation](https://public.tableau.com/app/profile/darrell.wolfe/viz/Capstone_MembervsCasual_Station_Usage/Distances "The Tableau Public Presentation")

------------------------------------------------------------------------

### Analysis Conclusions

------------------------------------------------------------------------

##### How do Members use Cyclistic vs Casual Riders?

In order to answer the ultimate question about "How do annual members and casual riders utilize Cyclistic bikes differently?", I asked the following sets of questions while doing my analysis.

1.  Ride Length: In each ride instance, who rides longer?
    -   Members have shorter rides.
2.  Who rides more often?
    -   Note: We can get a total members vs casual, but not a rider by rider with this data.
    -   Members do not ride as far as casual riders on their individual rides instances.
3.  Who rides farthest distance?
    -   Casual rides are farthest on given individual ride instances
    -   Members farther in total miles cumulatively.
4.  Are there opportunities for new stations that could increase new memberships?
    -   Specifically, are members frequently stopping and starting at particular non-station hot-spots?
    -   Actually - The data told a new story. Rather than focus on "new stations", I think we should focus on building awareness at existing stations that meet the criteria of how members use them.
    -   Members mostly use stations inside the city whereas casual riders use them near tourist locations. Focus the ad campaign on the city, specifically in higher income areas.
    -   Later, if we find this method of targeting casual riders who fit member patterns, we could expand the campaign to target new station areas that fit these patterns, but this would not be a wise use of investment at this stage.
5.  Total Rides and Total Rides by Member vs Casual
    -   There were over 8 million individual rides between January 2022 and July 2023.
    -   Of that total, 60.50% were Member rides, riding 4.9 million rides over casual user's 3.2 million rides.
    -   Extrapolating from the previous data, if Casual riders ride longer rides but Member riders make up far more total distance, then Member riders are riding far more often but for shorter distances.
    -   This means, we can target casual riders who fit the pattern of frequent shorter rides as a prime target for the campaign.
6.  Are there time of day preferences?
    -   Members and casual riders follow a similar pattern of a slow increase in the morning, maxing at mid-day and decrease into the evening.
    -   However, Members are significantly more active than casual members between 6a and 8a.
7.  Are they day of week preferences?
    -   Members ride more often on weekdays.
    -   Casual riders more often on weekends.
8.  Are there month of year/season of year preferences?
    -   By viewing rides over time, there is a steep drop off in total use during winter months an increase in spring, leading to a max in summer and decrease in fall.
    -   That being said, members do continue to use the service in winter at a higher rate than casual members.

------------------------------------------------------------------------

## Share

##### Guiding questions

-   Were you able to answer the question of how annual members and casual riders use Cyclistic bikes differently?
-   What story does your data tell?
-   How do your findings relate to your original question?
-   Who is your audience? What is the best way to communicate with them?
-   Can data visualization help you share your findings?
-   Is your presentation accessible to your audience?

##### Key tasks

1.  Determine the best way to share your findings.
2.  Create effective data visualizations.
3.  Present your findings.
4.  Ensure your work is accessible.

##### *Deliverable:Supporting visualizations and key findings*

------------------------------------------------------------------------

#### Summarizing the findings

The data for Cyclistic shows that there were over 8 million individual rides between January 2022 and July 2023, centering in in Chicago IL.

Of that total, 60.50% were Member rides, riding 4.9 million rides over casual user's 3.2 million rides.

##### Distance and time

Casual riders go farther given individual ride instances; however, Members ride farther in total miles cumulatively.

Members mostly use stations inside the city but in affluent areas; whereas, casual riders use them near tourist locations, specifically near the water in Chicago.

##### Date and time

Members and casual riders follow a similar pattern of a slow increase in the morning, maxing at mid-day and decrease into the evening. However, Members are significantly more active than casual members between 6a and 8a.

Members ride more often on weekdays.Casual riders more often on weekends.

Members ride more often than Casual riders but for shorter distances, and their highest usage is in Summer(June, July, and August), with a taper effect on either side, in Spring (May) and in Fall (September). That being said, members do continue to use the service in winter at a higher rate than casual members.

------------------------------------------------------------------------

##### Recommendations

Extrapolating from these findings, if Casual riders ride longer rides but Member riders make up far more total distance, then Member riders are riding far more often but for shorter distances.

This means, we can target casual riders who fit the pattern of frequent shorter rides as a prime target for the campaign.

We should focus the ad campaign on the inner city regions, specifically in higher income areas.

Later, if we find this method of targeting casual riders who fit member patterns effective, we could expand the campaign to target new station areas that fit these patterns, or even develop new stations not currently used by members, but this would be a premature investment at this stage.

------------------------------------------------------------------------

## Act

##### Guiding questions

-   What is your final conclusion based on your analysis?
-   How could your team and business apply your insights?
-   What next steps would you or your stakeholders take based on your findings?
-   Is there additional data you could use to expand on your findings?

##### Key tasks

1.  Create your portfolio.
2.  Add your case study.
3.  Practice presenting your case study to a friend or family member.

##### *Deliverable:Your top three recommendations based on your analysis*

------------------------------------------------------------------------

For my conclusions see above and the following link to my profile and case study:

### [Portfolio: Data Analyst, Data Visualization, Data Storytelling](https://www.toposcreative.com/p/portfolio.html "Portfolio: Data Analyst, Data Visualization, Data Storytelling")

[Cyclistic Membership Campaign \| A Coursera Capstone Case-Study, Track_1,Case_study_1, Cyclistic-bike-share-analysis](http://www.toposcreative.com/p/cyclistic-membership-campaign-coursera.html "Cyclistic Membership Campaign | A Coursera Capstone Case-Study, Track_1,Case_study_1, Cyclistic-bike-share-analysis")

# That concludes this presentation of the data.
